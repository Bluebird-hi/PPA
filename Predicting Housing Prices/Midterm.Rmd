---
title: "Predicting Housing Prices"
subtitle: "Philadelphia, 2022"
author: "Guangze Sun & Luming Xu"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    theme: spacelab
    toc_float: true
    toc_depth: 2
    code_folding: hide
    number_sections: true
bibliography: references.bib
---

<style type="text/css">

body, td {
   font-size: 16px;
}
code.r{
  font-size: 14px;
}
pre {
  font-size: 14px
}
</style>

```{r setup, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE) 
options(scipen=999)

# Load some libraries
library(tidyverse)
library(tidycensus)
library(stringr)
library(dplyr)
library(lubridate)
library(sf)
library(stargazer)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot)
library(corrr)      
library(kableExtra)
library(jtools)     
library(ggstance) 
library(ggpubr)    
library(broom.mixed) 
library(glue)
library(classInt)
library(viridis)
library(patchwork)
```

```{r setup2, message = FALSE}

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

palette5 <- colorRampPalette(c("#6D9EC1", "#E46726"))(5)
```

# Introduction

+ What is the purpose of this project?
+ Why should we care about it?
+ What makes this a difficult exercise?
+ What is your overall modeling strategy?
+ Briefly summarize your results.

# Data

## Data Wrangling

+ Briefly describe your methods for gathering the data.

Data Sources: [Philly neighborhoods (nhoods)](https://opendataphilly.org/datasets/philadelphia-neighborhoods/), [housing sale prices (studentData)](https://github.com/mafichman/musa_5080_2024/blob/main/Midterm/data/2023/studentData.geojson), [crime incidents (phillyCrimes)](https://metadata.phila.gov/#home/datasetdetails/5543868920583086178c4f8e/representationdetails/570e7621c03327dc14f4b68d/)

```{r data, echo = FALSE}
nhoods <- 
  st_read("data/philadelphia-neighborhoods.geojson") %>%
  st_transform('EPSG:2272')

studentData <- 
  st_read("data/studentData.geojson") %>%
  st_transform('EPSG:2272')

# school <- 
#   st_read("https://opendata.arcgis.com/datasets/d46a7e59e2c246c891fbee778759717e_0.geojson") %>%
#   st_transform('EPSG:2272')
# 
# hospital <- 
#   st_read("data/DOH_Hospitals202311.geojson") %>%
#   st_transform('EPSG:2272')
# 
# metro <- 
#   st_read("https://opendata.arcgis.com/api/v3/datasets/af52d74b872045d0abb4a6bbbb249453_0/downloads/data?format=geojson&spatialRefId=4326") %>%
#   st_transform('EPSG:2272')
# 
# trolley <- 
#     st_read("https://opendata.arcgis.com/api/v3/datasets/dd2afb618d804100867dfe0669383159_0/downloads/data?format=geojson&spatialRefId=4326") %>%
#   st_transform('EPSG:2272')
# 
# park <- 
#   st_read("https://opendata.arcgis.com/datasets/d52445160ab14380a673e5849203eb64_0.geojson") %>%
#   st_transform('EPSG:2272')
# 
# water <- 
#   st_read("https://services.arcgis.com/fLeGjb7u4uXqeF9q/arcgis/rest/services/Hydrographic_Features_Poly/FeatureServer/1/query?outFields=*&where=1%3D1&f=geojson") %>%
#   st_transform('EPSG:2272')
# 
# retail <- 
#   st_read("https://opendata.arcgis.com/datasets/53b8a1c653a74c92b2de23a5d7bf04a0_0.geojson") %>%
#   st_transform('EPSG:2272')
# 
# census_tract <- 
#   st_read("data/tl_2022_42_tract/tl_2022_42_tract.shp") %>%
#   st_transform('EPSG:2272')
# 
# census <- 
#   read.csv("data/census.csv")

descriptions <- read.csv("data/data dictonary-studentdata.csv")

philly <- studentData[,c(74,75,48,11,16,17,18,20,22,26,34,35,44,49,58,60,69,72,76)]
descriptions <- descriptions %>%
  filter(Field.Name %in% colnames(philly))
```

```{r data_process}
philly <- philly %>%
  mutate(PricePerSq = philly$sale_price / philly$total_livable_area) %>%
  #mutate(Age = 2024 - year_built) %>%
  #dplyr::select(- year_built) %>%
  #mutate(Age = case_when(Age == 2024 ~ mean(Age, na.rm = T),
  #      TRUE ~ Age)) %>%
  st_join(nhoods["NAME"])

#descriptions <- rbind(descriptions,
#                      data.frame(
#                        Field.Name = "Age", Alias = "Age", Description = "The age of housings"
#                      ))
#descriptions <- descriptions %>%
#  filter(Field.Name != "year_built")
```

## Feature Engineering

**Crimes: 2023-2019 (xlm)**

the ‘average nearest neighbor distance’ from each home sale to its k nearest neighbor crimes

```{r crimes_data, include = FALSE}
# phillyCrimes_2023 <- read.csv("data/Crimes/2023incidents_part1_part2.csv") %>%
#   dplyr::select(text_general_code, dispatch_date, lat, lng)
# phillyCrimes_2022 <- read.csv("data/Crimes/2022incidents_part1_part2.csv") %>%
#   dplyr::select(text_general_code, dispatch_date, lat, lng)
# phillyCrimes_2021 <- read.csv("data/Crimes/2021incidents_part1_part2.csv") %>%
#   dplyr::select(text_general_code, dispatch_date, lat, lng)
# phillyCrimes_2020 <- read.csv("data/Crimes/2020incidents_part1_part2.csv") %>%
#   dplyr::select(text_general_code, dispatch_date, lat, lng)
# phillyCrimes_2019 <- read.csv("data/Crimes/2019incidents_part1_part2.csv") %>%
#   dplyr::select(text_general_code, dispatch_date, lat, lng)
# 
# phillyCrimes <-
#   rbind(phillyCrimes_2023,phillyCrimes_2022,phillyCrimes_2021,phillyCrimes_2020,phillyCrimes_2019) %>%
#   mutate(dispatch_year = year(ymd(dispatch_date))) %>%
#   dplyr::select(lat, lng) %>%
#     na.omit() %>%
#     st_as_sf(coords = c("lng", "lat"), crs = "EPSG:4326") %>%
#     st_transform('EPSG:2272') %>%
#     distinct()
#
# st_write(phillyCrimes, dsn = 'data/phillyCrimes.geojson')

phillyCrimes <- 
  st_read("data/phillyCrimes.geojson")
```

```{r crimes_process}
philly$crimes.Buffer <- philly %>% 
    st_buffer(660) %>% 
    aggregate(mutate(phillyCrimes, counter = 1),., sum) %>%
    pull(counter)
philly <-
  philly %>% 
    mutate(
      crime_nn1 = nn_function(st_coordinates(philly), 
                              st_coordinates(phillyCrimes), k = 1),
      
      crime_nn2 = nn_function(st_coordinates(philly), 
                              st_coordinates(phillyCrimes), k = 2), 
      
      crime_nn3 = nn_function(st_coordinates(philly), 
                              st_coordinates(phillyCrimes), k = 3), 
      
      crime_nn4 = nn_function(st_coordinates(philly), 
                              st_coordinates(phillyCrimes), k = 4), 
      
      crime_nn5 = nn_function(st_coordinates(philly), 
                              st_coordinates(phillyCrimes), k = 5)) 
```

**Other data (sgz)**

```{r}
# city_hall <- metro[metro$Station == 'City Hall', 6]
# school_elem <- school %>%
#   filter(str_detect(GRADE_LEVEL, "ELEMENTARY"))
# 
# census <- census %>%
#   mutate(across(3:8, ~ as.numeric(as.character(.)))) %>%
#   mutate(NAMELSAD = str_extract(NAME, "^[^;]+")) %>%
#   select(NAMELSAD, 3:8)
# 
# calculate_nearest_distance <- function(set_points, other_layer) {
#   nearest_idx <- st_nearest_feature(set_points, other_layer)
#   st_distance(set_points, other_layer[nearest_idx, ], by_element = TRUE) %>% as.numeric()
# }
# set2 <- studentData[,74:76] %>%
#   mutate(distance_to_city_hall = st_distance(., city_hall) %>% as.numeric()) %>%
#   mutate(
#     distance_to_nearest_metro = calculate_nearest_distance(geometry, metro),
#     distance_to_nearest_hospital = calculate_nearest_distance(geometry, hospital),
#     distance_to_nearest_school = calculate_nearest_distance(geometry, school_elem),
#     distance_to_nearest_park = calculate_nearest_distance(geometry, park),
#     distance_to_nearest_water = calculate_nearest_distance(geometry, water)
#   )
# set2 <- set %>%
#   st_join(census_tract["NAMELSAD"]) %>%
#   left_join(census, by = "NAMELSAD") %>% 
#   st_join(retail[, c("LPSS_PER1000", "HPSS_PER1000")])
```


```{r}
set <- read.csv("data/set.csv")
philly_set <- merge(philly, set, by = "musaID", all.x = FALSE, all.y = FALSE, sort = FALSE) %>%
  dplyr::select(-toPredict.y) %>%
  rename(toPredict = toPredict.x) %>%
  mutate(dist1 = ifelse(distance_to_city_hall <= 20000, distance_to_city_hall, 20000))
philly <- philly_set %>% select(-distance_to_nearest_metro, -distance_to_nearest_hospital, -distance_to_nearest_school, -distance_to_nearest_park, -NAMELSAD, -Percent_Bachelor.s.degree.or.higher, -Percent_Management..business..science..and.arts.occupations, -Percent_Different.state, -Percent_With.private.health.insurance, -HPSS_PER1000)
```


## Exploratory analysis

```{r}
model_philly <- philly %>%
  filter(toPredict == "MODELLING")
```

+ Develop 1 map of your dependent variable (sale price)

```{r price_map}
ggplot() +
  geom_sf(data = nhoods, fill = "grey90", na.rm = TRUE) +
  geom_sf(data = model_philly, aes(colour = q5(sale_price)), 
          show.legend = "point", size = 0.75) +
  scale_colour_manual(values = palette5,
                      labels = qBr(model_philly, "sale_price"),
                      name = "Quintile\nBreaks") +
  labs(title = "Sale Price, Philadelphia") +
  theme_void()
```

+ Present a table of summary statistics with variable descriptions. Sort these variables by their category (internal characteristics, amenities/public services or spatial structure).

Internal characteristics: bed, bath, area, age, garage, floor...

Amenities/public services: crime, distance to CBD/railway stations/schools/hospitals/grocery stores, green space...

The spatial process of prices: education, income, white...

(示例) references [@dubin1998predicting]

```{r}
stat_philly <- philly %>%
  dplyr::select(-musaID, -sale_price, -PricePerSq, -crimes.Buffer, -crime_nn1, -crime_nn2, -crime_nn3, -crime_nn4)

descriptions_selected <- descriptions %>%
  filter(!Field.Name %in% c("sale_price")) %>%
  dplyr::select(Alias, Description)
print(descriptions_selected)

stargazer(st_drop_geometry(stat_philly), 
          type = 'text', 
          title = "Summary Statistics",
          summary.stat = c("mean", "sd", "min", "max", "n"))
```

+ Include any other maps/graphs/charts you think might be of interest.

```{r}
numericVars <- 
  select_if(st_drop_geometry(model_philly), is.numeric) %>% na.omit() %>%
  dplyr::select(-musaID, -PricePerSq, -crimes.Buffer, -crime_nn1, -crime_nn2, -crime_nn3, -crime_nn4)

ggplot(gather(numericVars), aes(value)) +
  geom_histogram(bins = 50) +
  facet_wrap(~key,nrow=4, scales = 'free_x') +
  theme_minimal() +
  theme(axis.text = element_blank(), axis.ticks = element_blank())
```


```{r}
# all numeric variables
model_philly_numeric <- st_drop_geometry(model_philly) %>% 
  dplyr::select(sale_price, crime_nn5, 
                frontage, total_livable_area, 
                distance_to_city_hall, distance_to_nearest_water, 
                Estimate_Mean.family.income..dollars.,Percent_White.alone,LPSS_PER1000
                ) %>%
  filter(sale_price <= 1000000) %>%
  gather(Variable, Value, -sale_price)

ggplot(model_philly_numeric, aes(Value, sale_price)) +
  geom_point(size = .5) + geom_smooth(method = "lm", se=F, colour = "#E46726") +
  facet_wrap(~Variable, ncol = 4, scales = "free") +
  labs(title = "Price as a function of continuous variables") +
  theme_minimal() +
  theme(axis.text = element_blank(), axis.ticks = element_blank())
  
```

```{r}
# 加载所需的库
library(ggplot2)
library(patchwork)

# 过滤数据并添加标签列
model_philly_numeric <- st_drop_geometry(model_philly) %>% 
  dplyr::select(sale_price, distance_to_city_hall) %>%
  filter(sale_price <= 1000000) %>%
  mutate(DistanceCategory = ifelse(distance_to_city_hall <= 20000, "<= 20000", "> 20000"))

# 左侧图（距离 <= 20000）
plot1 <- ggplot(subset(model_philly_numeric, DistanceCategory == "<= 20000"),
                aes(distance_to_city_hall, sale_price)) +
  geom_point(size = 0.5, color = "black") +
  geom_smooth(method = "lm", se = FALSE, color = "#E46726") +
  ylab("Sale Price") +
  theme_minimal() +
  theme(axis.title.x = element_blank())  # 去除 x 轴标签

# 右侧图（距离 > 20000）
plot2 <- ggplot(subset(model_philly_numeric, DistanceCategory == "> 20000"),
                aes(distance_to_city_hall, sale_price)) +
  geom_point(size = 0.5, color = "black") +
  geom_smooth(method = "lm", se = FALSE, color = "#E46726") +
  theme_minimal() +
  theme(axis.title.x = element_blank(),  # 去除 x 轴标签
        axis.title.y = element_blank(),  # 去除 y 轴标签
        axis.text.y = element_blank(),   # 去除 y 轴刻度
        axis.ticks.y = element_blank())  # 去除 y 轴刻度线

# 组合图，并设置整体标题
combined_plot <- plot1 + plot2 + 
  plot_layout(widths = c(1, 4)) + 
  plot_annotation(
    title = "Price as a function of Distance to City Hall",
    theme = theme(plot.title = element_text(size = 14, hjust = 0.5))
  )

# 显示组合图
combined_plot

```


```{r}
# all factor variables
model_philly_factor <- st_drop_geometry(model_philly) %>% 
  dplyr::select(sale_price, 
                central_air, exterior_condition, fireplaces, garage_spaces, general_construction,
                interior_condition, number_of_bathrooms, number_of_bedrooms, quality_grade, separate_utilities,
                topography, zoning, building_code_description_new) %>%
  mutate(exterior_condition = as.factor(exterior_condition),
         fireplaces = as.factor(fireplaces),
         garage_spaces = as.factor(garage_spaces),
         interior_condition = as.factor(interior_condition),
         number_of_bathrooms = as.factor(number_of_bathrooms),
         number_of_bedrooms = as.factor(number_of_bedrooms)) %>%
  filter(sale_price <= 1000000) %>%
  gather(Variable, Value, -sale_price)

ggplot(model_philly_factor, aes(Value, sale_price)) +
  geom_bar(position = "dodge", stat = "summary", fun.y = "mean") +
  facet_wrap(~Variable, ncol = 4, scales = "free") +
  labs(title = "Price as a function of categorical variables", y = "Mean Price") +
  theme_minimal() +
  theme(axis.text = element_blank(), axis.ticks = element_blank())
```

```{r}
## Crime cor
model_philly %>%
  st_drop_geometry() %>%
  dplyr::select(sale_price, starts_with("crime")) %>%
  filter(sale_price <= 1000000) %>%
  gather(Variable, Value, -sale_price) %>% 
   ggplot(aes(Value, sale_price)) +
     geom_point(size = .5) + geom_smooth(method = "lm", se=F, colour = "#E46726") +
     facet_wrap(~Variable, nrow = 1, scales = "free") +
     labs(title = "Price as a function of crimes") +
     theme_minimal()
```

+ Present a correlation matrix

this plot shows features that may be colinear, such as ...
(These two features are ‘colinear’, or correlated with one another, so if both are input into the regression, one is insignificant. In such an instance, retain the feature that leads to a more accurate and generalizable model.)

```{r}
ggcorrplot(
  round(cor(numericVars), 1), 
  p.mat = cor_pmat(numericVars),
  colors = c("#6D9EC1", "white", "#E46726"),
  type="lower",
  insig = "blank", outline.col = "grey90") +  
    labs(title = "Correlation across numeric variables") +
  theme(axis.text = element_text(size = 1))
```

(exploratory analysis goes beyond just correlation. Keep in mind that good exploratory analysis adds valuable context, particularly for non-technical audiences. )

## Feature Selection

(feature engineering like recoding Style into fewer categories, or converting NUM_FLOORS from numeric to categorical, could lead to a significant improvement in a predictive model.)

+ Present 4 home price correlation scatterplots that you think are of interest. I’m going to look for interesting open data that you’ve integrated with the home sale observations.

```{r}
st_drop_geometry(model_philly) %>% 
  dplyr::select(sale_price, total_livable_area, crime_nn5, distance_to_city_hall, Estimate_Mean.family.income..dollars.) %>%
  filter(sale_price <= 2000000 & crime_nn5<=1000 & total_livable_area<=7500) %>%
  gather(Variable, Value, -sale_price) %>%
  ggplot(aes(Value, sale_price)) +
  geom_point(size = .5) + geom_smooth(method = "lm", se=F, colour = "#FA7800") +
  facet_wrap(~Variable, ncol = 2, scales = "free") +
  labs(title = "Price as a function of continuous variables") +
  theme_minimal()
```

+ Develop 3 maps of 3 of your most interesting independent variables.

```{r three_maps}
ggplot() +
  geom_sf(data = nhoods, fill = "transparent", color ="grey", na.rm = T) +
  geom_sf(data = philly, aes(colour = q5(LPSS_PER1000), na.rm = T), 
          show.legend = "point", size = .75) +
  scale_colour_manual(values = palette5,
                   labels=qBr(philly,"LPSS_PER1000"),
                   name="Quintile\nBreaks") +
  labs(title="Low Produce Stores, Philadelphia") +
  theme_void()

ggplot() +
  geom_sf(data = nhoods, fill = "transparent", color ="grey", na.rm = T) +
  geom_sf(data = philly, aes(color = as.factor(number_of_bathrooms), na.rm = T), 
          show.legend = "point", size = .75) +
  scale_color_viridis(discrete = TRUE, option = "D")+
  labs(title="Number of Bathrooms, Philadelphia") +
  theme_void()

ggplot() +
  geom_sf(data = nhoods, fill = "transparent", color ="grey", na.rm = T) +
  geom_sf(data = philly, aes(color = zoning, na.rm = T), 
          show.legend = "point", size = .75) +
  #scale_color_brewer(palette = "RdYlBu",direction = -1)+
  labs(title="Zoning, Philadelphia") +
  theme_void()

```

**?? pearson correlation**


```{r}
# cor_philly <- 
#   st_drop_geometry(model_philly) %>%
#   filter(sale_price <= 1000000,
#          total_livable_area < 10000) %>%
#   dplyr::select(sale_price, total_livable_area)
# 
# cor.test(cor_philly$total_livable_area,
#          cor_philly$sale_price, 
#          method = "pearson")
# 
# 
# # Calculate predicted prices
# cor_philly$predicted_price <- predict(lm(sale_price ~ total_livable_area, data = cor_philly), newdata = cor_philly)
# ggscatter(cor_philly,
#           x = "total_livable_area",
#           y = "sale_price",
#           color = "#6D9EC1", size = 1, alpha = 0.6) +
#   geom_point(aes(y = predicted_price), color = "#E46726", size = 1, alpha = 0.6) +
#   stat_cor(label.x = 4000, label.y = 250000, hjust = 0) +
#   labs(title = "Price as a function of living area", 
#        subtitle = "With predicted prices; Sale prices <= $1 mil.",
#        x = "Total Livable Area",
#        y = "Sale Price") +
#   theme_minimal() +
#   theme(plot.title = element_text(size = 18, face = "bold"),
#         plot.subtitle = element_text(size = 12)) 
```

## data再处理

```{r}
philly_fac <- philly %>%
  mutate(
    central_air = case_when(
      central_air %in% c("1", "Y") ~ "Y",  
      TRUE ~ "N"                          
    ),
    central_air = factor(central_air),
    
    exterior_condition = case_when(
      is.na(exterior_condition) | exterior_condition == 1 ~ "1",    
      exterior_condition %in% c(2, 3) ~ "2",                        
      exterior_condition == 4 ~ "3",                                
      TRUE ~ "4"                      
    ),
    exterior_condition = factor(exterior_condition),
    
    fireplaces = case_when(
      is.na(fireplaces) | fireplaces == 0 | fireplaces == 1 ~ "0",                    
      TRUE ~ "1"                                
    ),
    fireplaces = factor(fireplaces),
    
    garage_spaces = case_when(
      is.na(garage_spaces) | garage_spaces == 0 ~ "0",    
      garage_spaces == 1 ~ "1",      
      TRUE ~ "2"                     
    ),
    garage_spaces = factor(garage_spaces),
    
    general_construction = case_when(       
      general_construction %in% c("A ", "B ", "D ") ~ "A",  
      general_construction == "3 " ~ "C",        
      TRUE ~ "B"                                 
    ),
    general_construction = factor(general_construction),
    
    interior_condition = case_when(
      interior_condition %in% c(0, 1, 2, 3) ~ "1",         
      interior_condition %in% c(4, NA) ~ "2",        
      TRUE ~ "3"       
    ),
    interior_condition = factor(interior_condition),
    
    number_of_bathrooms = case_when(
      number_of_bathrooms == 0 ~ "0",                   
      number_of_bathrooms == 1 ~ "1",                   
      number_of_bathrooms %in% c(2, NA) ~ "2",          
      number_of_bathrooms == 3 ~ "3",                   
      TRUE ~ "4"                                        
    ),
    number_of_bathrooms = factor(number_of_bathrooms),
    
    number_of_bedrooms = case_when(
      number_of_bedrooms %in% c(0, 31) | is.na(number_of_bedrooms) ~ "0",  
      number_of_bedrooms %in% c(1, 2) ~ "1",    
      number_of_bedrooms == 3 ~ "2",            
      number_of_bedrooms == 4 ~ "3",            
      number_of_bedrooms %in% c(5, 6) ~ "4",    
      number_of_bedrooms %in% c(7, 8, 9) ~ "5" 
    ),
    number_of_bedrooms = factor(number_of_bedrooms),
    
    quality_grade = case_when(
      quality_grade %in% c("A+", "A", "A-", "X", "X-") ~ "1",           
      quality_grade %in% c("B+", "B", "B-", "3", "S+") ~ "2",
      TRUE ~ "3"                                             
    ),
    quality_grade = factor(quality_grade),
    
    separate_utilities = case_when(
      separate_utilities %in% c("A", "C") ~ "B",  
      TRUE ~ "A"                                 
    ),
    separate_utilities = factor(separate_utilities),
    
    topography = case_when(
      topography == "B" ~ "B",
      TRUE ~ "A"
    ),
    topography = factor(topography),
    
    building_code_description_new = case_when(
      building_code_description_new %in% c("COLONIAL", "OLD STYLE", "ROW MODERN", "ROW OLD STYLE", "TUDOR", "TWIN BUNGALOW") ~ "1",
      building_code_description_new %in% c("ROW POST WAR", "ROW TYPICAL", "TWIN CONVENTIONAL") ~ "3",
      building_code_description_new %in% c("OTHER", "ROW PORCH FRONT") ~ "4",
      TRUE ~ "2"
    ),
    building_code_description_new = factor(building_code_description_new),
    
    NAME = case_when(
      NAME %in% c("CHINATOWN", "CRESTMONT_FARMS", "EAST_PARK", "MECHANICSVILLE", 
                  "PENNYPACK_PARK", "UNIVERSITY_CITY", "WISSAHICKON_PARK", "WOODLAND_TERRACE") ~ "OTHERS",
      TRUE ~ NAME 
    ),
    NAME = factor(NAME)
  )

philly <- philly_fac
model_philly <- philly %>%
  filter(toPredict == "MODELLING")
```


# Methods

Briefly describe your method (remember who your audience is).

a)  data cleaning
b)	Exploratory Data Analysis
c)  OLS Regression
d)  additional analyses (k-fold cross-validation, Moran's I...)
e)  software

# Results

Using the features selected above, our model explains 76% of the variation in price and produces average errors of 36% relative to housing prices. In terms of cross-validation, our model was well-trained on the available variables and shows high accuracy for low-priced houses. However, it demonstrates less consistency and a slight positive spatial autocorrelation, suggesting the influence of unaccounted factors. At the urban scale, our model is less effective at predicting prices in low-income neighborhoods and reveals a significant discrepancy between high- and low-income areas, indicating a further development in generalizability.

## Our Regression Model: explaining 76% of the prices

To improve the model's accuracy and generalizability, we randomly split the dataset into 60% for training the model and 40% for testing its goodness of fit. Using the selected features, OLS regression was performed on the training set.

```{r}
set.seed(34)
inTrain <- createDataPartition(
              y = paste(model_philly$general_construction, 
                       model_philly$topography,
                       model_philly$quality_grade,
                        model_philly$NAME),
                        
              p = .60, list = FALSE)
# [row,column] select all columns
philly.training <- model_philly[inTrain,] 
philly.test <- model_philly[-inTrain,]
```
```{r}
reg.training <- lm(sale_price ~ log(total_livable_area+1) + total_livable_area + crime_nn5 + LPSS_PER1000 +
              log(dist1+1) +log(distance_to_city_hall+1) + log(distance_to_nearest_water+1) +
              Estimate_Mean.family.income..dollars. + Percent_White.alone + log(frontage+1) +  
              
              central_air + exterior_condition + fireplaces + garage_spaces + general_construction + 
              interior_condition + number_of_bathrooms + number_of_bedrooms + quality_grade + 
              separate_utilities + topography + building_code_description_new + zoning + 
              NAME,
    data = st_drop_geometry(philly.training))
```
```{r}
kable(summary(reg.training)$coefficients, digits = 3,format = "html",caption = "OLS Regression Results", align = "ccccc") %>%
  kableExtra::kable_material(lightable_options = c("striped", "hover")) %>%
  scroll_box(height = "300px")
```


In our model, the features explain approximately 76% of the variation in price (Adjusted R^2^ = `r scales::percent(summary(reg.training)$adj.r.squared, accuracy = 0.01)`) and most of their coefficients are statistically significant (p<0.1). Several features, such as general construction and architectural styles, are converted to categorical variables, allowing their coefficients to be estimated and linked to housing prices. The remaining insignificant coefficients are of neighborhood names or zoning numbers, which are less likely to regroup due to spatial disparities.

## Accuracy: better for sales priced below $1,000,000

To dictate how useful the model is for decision making, we used testing set to analyse goodness of fit indicators.

```{r}
philly.test <-
  philly.test %>%
  mutate(Regression = "Baseline Regression",
         sale_price.Predict = predict(reg.training, philly.test),
         sale_price.Error = sale_price.Predict - sale_price,
         sale_price.AbsError = abs(sale_price.Predict - sale_price),
         sale_price.APE = (abs(sale_price.Predict - sale_price)) / sale_price) %>%
  filter(sale_price < 5000000)

MAE <- mean(philly.test$sale_price.AbsError, na.rm = T)
MAPE <- mean(philly.test$sale_price.APE, na.rm = T)
acc <- data.frame(MAE, MAPE)

acc %>%
  knitr::kable(caption = "MAE and MAPE of the testing dataset", align = "cc") %>%
  kableExtra::kable_material(lightable_options = c("striped", "hover"))
```

The Mean Absolute Error (MAE) is `r round(MAE,2)`, representing the average difference between predicted and observed prices. The relatively high value reflects the diversity in housing prices, suggesting a limitation for linear regression. The Mean Absolute Percent Error (MAPE) indicates that the prediction error amounts to `r scales::percent(MAPE, accuracy = 0.01)` of the housing prices on average.

```{r}
ggplot(
  philly.test, aes(sale_price.Predict, sale_price)) +
  geom_point(size = .5) + 
  geom_smooth(method = "lm", se=F, colour = "#6D9EC1") +
  geom_abline(intercept = 0, slope = 1, color="#E46726",size=1) +
  theme_minimal() +
  theme(plot.title = element_text(size = 18, face = "bold"),
        plot.subtitle = ggtext::element_markdown(size = 12)) +
  labs(title = 'Predicted sale price as a function of observed price', 
       subtitle = glue("<span style='color:#E46726;'>Perfect prediction</span> vs. <span style='color:#6D9EC1;'>Average prediction</span>"),
       x = "Predicted Sale Price",
       y = "Observed Sale Price")
```

Given the high MAE and MAPE, data visualization was used to compare predicted and observed prices. The red and blue lines nearly overlap, indicating the model performs well overall. However, the slight deviation suggests that, on average, the model's predictions are a little higher than the observed prices. Examining the data points further reveals that prediction accuracy decreases as prices rise, confirming our concerns about the variability in the higher price ranges of the dataset. This suggests that our model performs particularly well for lower-valued housing prices.

## Generalizability: cross-validation shows less consistency

After making predictions on a single hold-out test set, we partitioned the dataset into 100 equal-sized subsets, training on the remaining data and testing on each subset. This process is known as the K-fold cross-validation method.

```{r}
fitControl <- trainControl(method = "cv", number = 100)
set.seed(825)

reg.cv <- 
  train(sale_price ~ log(total_livable_area+1) + total_livable_area + crime_nn5 + LPSS_PER1000 +
              log(dist1+1) +log(distance_to_city_hall+1) + log(distance_to_nearest_water+1) +
              Estimate_Mean.family.income..dollars. + Percent_White.alone + log(frontage+1) +  
              
              central_air + exterior_condition + fireplaces + garage_spaces + general_construction + 
              interior_condition + number_of_bathrooms + number_of_bedrooms + quality_grade + 
              separate_utilities + topography + building_code_description_new + zoning + 
              NAME,
    
    data = st_drop_geometry(model_philly),
    method = "lm", trControl = fitControl, na.action = na.pass)
```

```{r}
kable(reg.cv$results, digits = 3,caption = "Cross-Validation Results", align = "ccccccc") %>%
  kableExtra::kable_material(lightable_options = c("striped", "hover"))
```


```{r}
Mean <- mean(reg.cv$resample[,3])
SD <- sd(reg.cv$resample[,3])
stat_MAE <- data.frame(Mean, SD)

stat_MAE %>%
  knitr::kable(caption = "Mean and standard deviation of the cross-validation MAE", align = "cc") %>%
  kableExtra::kable_material(lightable_options = c("striped", "hover"))
```

The standard deviation of MAE (`r round(SD,2)`) reflects significant variation across the 100 folds. Additionally, the mean of MAE in cross-validation (`r round(Mean,2)`) is quite similar to the MAE in our test set (`r round(MAE,2)`), indicating that the model was well-trained on the existing variables and therefore shows an average error.

```{r}
ggplot(data = data.frame(mae = reg.cv$resample[,3]),aes(x = mae)) +
  geom_histogram(color = "white", fill = "#6D9EC1") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 50),
                     labels = function(x) ifelse(x %% 5000 == 0, x, "")) +
  labs(title = "Distribution of MAE", 
       subtitle = "K fold cross validation; k=100.",
       x = "Mean Absolute Error", y = "Count") +
  theme_minimal() +
  theme(plot.title = element_text(size = 18, face = "bold"),
        plot.subtitle = element_text(size = 12)) +
  geom_vline(aes(xintercept=mean(mae, na.rm=T)),
               color="#E46726", linetype="dashed", size=1) +
  geom_vline(aes(xintercept= MAE),
               color="#E46726", linetype="dashed", size=.8, alpha = .8) +
  annotate(geom = "text", label = paste("Mean of MAE:", round(Mean, 2)), # Insert correlation here
    x = 67000, y = 10, 
    hjust = 0, size = 4, color = "#E46726") +
  annotate(geom = "text", label = paste("MAE in the test set:\n", round(MAE, 2)), # Insert correlation here
    x = 61000, y = 9, 
    hjust = 1, size = 4, color = "#E46726", alpha = .8) 
```

Visualized in a histogram, the distribution of MAE in cross-validation peaks around 63000 and exhibits a long right tail. This suggests that the model predicts inconsistently and may be unreliable for predicting houses that have not been sold recently.

## Generalizability: positive spatial autocorrelation exists

```{r}
philly.test_predict <-
  philly.test[which(philly.test$sale_price.Error != 0),]

coords.test <-  st_coordinates(philly.test_predict) 

neighborList.test <- knn2nb(knearneigh(coords.test, 5))

spatialWeights.test <- nb2listw(neighborList.test, style="W")
philly.test_predict$lagPriceError <- lag.listw(spatialWeights.test, philly.test_predict$sale_price.Error)

philly.test <- philly.test_predict
```

```{r}
ggplot() +
  geom_sf(data = nhoods, fill = "grey40", color = "grey", na.rm = T) +
  geom_sf(data = philly.test_predict, aes(colour = q5(sale_price.Error), na.rm = T),
          show.legend = "point", size = .75) +
  scale_colour_manual(values = palette5,
                   labels=qBr(philly.test_predict,"sale_price.Error"),
                   name="Quintile Breaks") +
  guides(color = guide_legend(override.aes = list(size = 3))) +
  labs(title="Sale price errors on the test set") +
  mapTheme() +
  theme(plot.title = element_text(size = 18, face = "bold"),
        plot.subtitle = element_text(size = 12),
        panel.border = element_rect(colour = "black", fill=NA, size=1))
```


What might be causing the high MAE? Although our model incorporates internal characteristics, public services/amenities, and some spatial features, other factors influencing the spatial distribution of home prices remain unexplored. As the map illustrates, sale price errors in Philadelphia are generally randomly distributed, but with some clustering both within and across neighborhoods. How much do our model errors cluster? Calculating the average weighted model error of its five nearest neighbors as the "spatial lag", we delved deeper into the spatial autocorrelation test.

```{r}
cor_error <- cor(philly.test_predict$lagPriceError, philly.test_predict$sale_price.Error, method = "pearson")
ggplot(philly.test_predict, aes(x = lagPriceError, y = sale_price.Error))+
     geom_point(size = .5, color = "#6D9EC1") + geom_smooth(method = "lm", se=F, colour = "#E46726") +
     labs(title = "Error as a function of the spatial lag of price",
          x = "Spatial lag of errors (Mean error of 5 nearest neighbors)", y = "Sale price errors") +
  annotate(geom = "text", label = paste("Pearson Correlation:", round(cor_error, 3)), # Insert correlation here
    x = -700000, y = -1000000, 
    hjust = 0, vjust = 2, 
    size = 4) +
     theme_minimal() +
  theme(plot.title = element_text(size = 18, face = "bold"))
```

Comparing the spatial lag with the actual values helps estimate the degree of spatial autocorrelation. The relationship visualized above shows that as home price errors increase, nearby home price errors tend to rise as well. The correlation is `r round(cor_error,3)`, which is marginal but significant.

```{r}
philly.test_nonzero <- philly.test %>%
  filter(sale_price.Error != 0)
moranTest <- moran.mc(philly.test_nonzero$sale_price.Error, 
                      spatialWeights.test, nsim = 999)

ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = moranTest$statistic), colour = "#E46726",size=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  labs(title="<span style='color:#E46726;'>Observed</span> and <span style='color:#7F7F7F;'>permuted</span> Moran's I",
       x="Moran's I",
       y="Count") +
  annotate(geom = "text", label = paste("Statistic:", round(moranTest$statistic,3),"\np-value:", moranTest$p.value), # Insert correlation here
    x = 0.2, y = 300, 
    hjust = 0, 
    size = 4) +
  theme_minimal() +
  theme(plot.title = ggtext::element_markdown(size = 18, face = "bold"))
```

Another approach to measure spatial autocorrelation is Moran's I, where a positive value close to one indicates strong positive spatial autocorrelation. The histogram above shows 999 randomly permuted I values, with the observed I marked by the orange line. The observed I, higher than all random permutations, confirms spatial autocorrelation. An I of `r round(moranTest$statistic,3)` seems marginal, but a p-value of `r moranTest$p.value` still indicates statistically significant clustering.

Both the spatial lag and Moran’s I test show that our model errors exhibit a slight positive spatial autocorrelation, suggesting the presence of unaccounted factors.


## Generalizability: less effective for low-priced neighborhoods and low-income tracts

How does the spatial autocorrelation manifest in our model? From the perspective of geospatial features, including neighborhoods and median income, we further analysed the spatial process of sale price predictions and tested the generalizability across urban contexts.

```{r}
nhoods_model <- st_drop_geometry(philly.test) %>%
  group_by(NAME) %>%
  summarise(mean.MAPE = mean(sale_price.APE, na.rm = T)) %>%
  ungroup() %>%
  left_join(nhoods) %>%
  st_sf()

ggplot() +
  geom_sf(data = nhoods, fill = "transparent", color = "grey") +
  geom_sf(data = nhoods_model, aes(fill = mean.MAPE), color = "grey") +
  scale_fill_gradient(low = "#6D9EC1", high = "#E46726", name = "MAPE") +
  labs(title = "Mean test set MAPE by neighborhood") +
  mapTheme() +
  theme(plot.title = element_text(size = 18, face = "bold"),
        plot.subtitle = element_text(size = 12),
        panel.border = element_rect(colour = "black", fill=NA, size=1))
```

The MAPE by neighborhood reveals significantly less accurate predictions in North and West Philadelphia (highlighted in red). The vacant areas have no housing sales data, including northern Center City, University City, Northeast Philadelphia Airport and its surroundings, southern South Philadelphia, and parks. In [North Philadelphia](https://en.wikipedia.org/wiki/North_Philadelphia#Neighborhoods), the red areas are predominantly home to Hispanic and Black residents, while the surrounding population is more diverse, contributing to the high MAPE. In [West Philadelphia](https://en.wikipedia.org/wiki/West_Philadelphia#History), the impact of the University of Pennsylvania’s role in gentrification in the east and rising crime in the west have led to deteriorating areas with high MAPE. Both red areas also have lower sale prices, confirming the spatial pattern of model generalizability.


```{r}
nhoods_model <- left_join(nhoods_model,
  st_drop_geometry(philly.test) %>%
    group_by(NAME) %>%
    summarise(meanPrice = mean(sale_price, na.rm = T))
) 
ggplot(data = st_drop_geometry(nhoods_model)) +
  geom_point(aes(x = meanPrice, y = mean.MAPE), size = 2, color = "#E46726", alpha = 0.7) +
  geom_vline(aes(xintercept= 375000),
               color="#6D9EC1", linetype="dashed", size=.8, alpha = .8) +
  geom_hline(aes(yintercept= .4),
               color="#6D9EC1", linetype="dashed", size=.8, alpha = .8) +
  labs(title = "MAPE as a function of mean price by neighborhood",
       x = "Mean Price by Neighborhood",
       y = "MAPE by Neighborhood") +
  theme_minimal() +
  theme(plot.title = element_text(size = 18, face = "bold")) +
  annotate(geom = "text", label = paste("(275000,0.4)"),
    x = 275000, y = 0.4, 
    hjust = -.8, vjust = -1, size = 4, color = "#6D9EC1", alpha = .8)
```

Upon further examining the relationship between price and MAPE across neighborhoods, it is evident that MAPE decreases as the mean price increases up to $375,000, but starts to rise when the mean price exceeds this threshold. In our model, mean prices between $125,000 and $500,000 are associated with relatively lower MAPE. The spatial pattern of MAPE (figure ) also shows that lower-priced, declining areas tend to have higher MAPE. These two graphs highlight that our model is more effective in neighborhoods with mid-range prices and that the socioeconomic context of neighborhoods impacts pricing.


```{r}
census_api_key("e62580f74ef222beadd9dd2fbaf48ff130b31c4a", overwrite = TRUE)
acs_variable_list.2022 <- load_variables(2022, #year
                                         "acs5", #five year ACS estimates
                                         cache = TRUE)
```
```{r include = FALSE}
tracts22 <- get_acs(geography = "tract",
                    variables = c("B01003_001E", "B01001A_001E","B06011_001E"), 
                    year=2022, 
                    state=42, 
                    county=101, 
                    geometry=TRUE, 
                    output="wide") %>%
  st_transform('EPSG:2272') %>%
  rename(TotalPop = B01003_001E, 
         TotalWhites = B01001A_001E,
         MedInc = B06011_001E) %>%
  dplyr::select(-NAME, -starts_with("B")) %>%
  mutate(year = "2022",
         percentWhite = TotalWhites / TotalPop,
         raceContext = ifelse(percentWhite > .5, "Majority White", "Majority Non-White"),
         incomeContext = ifelse(MedInc > mean(MedInc, na.rm = T), "High Income", "Low Income"))
```

```{r}
ggplot() + 
  geom_sf(data = tracts22, fill = "transparent", color = "grey") +
  geom_sf(data = na.omit(tracts22), aes(fill = incomeContext), color = "grey90") +
  scale_fill_manual(values = c("#E46726", "#6D9EC1"), name="Income Context") +
  labs(title = "Tracts grouped by income",
       subtitle = "Philadelphia, 2022") +
  mapTheme() + 
  theme(plot.title = element_text(size = 18, face = "bold"),
        plot.subtitle = element_text(size = 12),
        panel.border = element_rect(colour = "black", fill=NA, size=1))
```

```{r}
income_table <- st_join(philly.test, tracts22) %>%
  group_by(incomeContext) %>%
  summarise(mean.MAPE = scales::percent(mean(sale_price.APE, na.rm = T))) %>%
  st_drop_geometry() %>%
  spread(incomeContext, mean.MAPE) %>%
  dplyr::select(-"<NA>") %>%
  mutate('No Context' = scales::percent(MAPE))

income_table %>%
  knitr::kable(caption = "Test set MAPE by tract income context", align = "ccc") %>%
  kableExtra::kable_material(lightable_options = c("striped", "hover"))
```

At the urban scale, we compared MAPE across different income contexts to assess the generalizability of our model. Tracts with median incomes above the citywide mean were designated as 'High Income', effectively dividing Philadelphia into two categories. The nearly 1:2 ratio in MAPE highlights a significant difference in the model’s goodness of fit, indicating higher accuracy in predicting housing prices in high-income areas. The neighborhoods with high MAPE (see Figure) fall predominantly within the low-income group, suggesting a connection between race, crime, income, and housing prices.

## Challenge time

Using our model, the predicted values for where ‘toPredict’ is both “MODELLING” and “CHALLENGE” are as follows.

```{r}
philly_finish <- philly %>%
   mutate(sale_price.Predict = predict(reg.training, philly))
```

```{r}
ggplot() +
  geom_sf(data = nhoods, fill = "grey40", color = "grey", na.rm = T) +
  geom_sf(data = philly_finish %>% filter(!is.na(sale_price.Predict)), aes(colour = q5(sale_price.Predict), na.rm = T),
          show.legend = "point", size = .75) +
  scale_colour_manual(values = palette5,
                   labels=qBr(philly_finish,"sale_price.Predict"),
                   name="Quintile Breaks") +
  guides(color = guide_legend(override.aes = list(size = 3))) +
  labs(title="Predicted Values of 'Modelling' and 'Challenge'",
       subtitle = "Philadelphia, 2022") +
  mapTheme() +
  theme(plot.title = element_text(size = 18, face = "bold"),
        plot.subtitle = element_text(size = 12),
        panel.border = element_rect(colour = "black", fill=NA, size=1))
```

# Discussion

(Is this an effective model? **What were some of the more interesting variables?**  How much of the variation in prices could you predict? Describe the more important features? Describe the error in your predictions?  According to your maps, could you account the spatial variation in prices?  Where did the model predict particularly well? Poorly? Why do you think this might be?)

Our model effectively predicts 76% of the variation in housing prices, particularly with high accuracy for sales priced below $1,000,000. Key features such as distance to the city center, neighborhood names, and zoning numbers significantly reduce prediction errors by approximately 20% when included in training. The Mean Absolute Error is `r round(MAE,2)` and the Mean Absolute Percent Error is `r scales::percent(MAPE, accuracy = 0.01)`. Prediction errors remain high due to the dataset's wide price variation and unaccounted factors influencing spatial dynamics. From the perspective of urban context, the model performs poorly in North Philadelphia, where clustering of Hispanic and Black residents presents challenges, and in West Philadelphia, where rising crime and gentrification add complexity. These decaying factors contribute to lower housing prices and a more intricate spatial process. Nonetheless, the model performs well in neighborhoods with mid-range housing prices, where the larger dataset provides stronger training and testing opportunities.

# Conclusion

(Would you recommend your model to Zillow? Why or why not? How might you improve this model?)

I would recommend our model to Zillow for customers looking to predict mid-range housing prices in Philadelphia. The model is well-trained and accounts for internal characteristics, public services/amenities, and spatial features, demonstrating a strong fit within the context of Philadelphia. While there is some spatial autocorrelation present and OLS regression has its limitations, we plan to explore additional socioeconomic factors that influence the spatial process of housing prices and apply multivariate regression techniques to further enhance the model.

# References





