---
title: "Predicting housing prices"
author: "Guangze Sun & Luming Xu"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    theme: spacelab
    toc_float: true
    toc_depth: 2
    code_folding: hide
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE) 
options(scipen=999)

# Load some libraries
library(tidyverse)
library(dplyr)
library(lubridate)
library(sf)
library(stargazer)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot) # plot correlation plot
library(corrr)      # another way to plot correlation plot
library(kableExtra)
library(jtools)     # for regression model plots
library(ggstance) # to support jtools plots
library(ggpubr)    # plotting R^2 value on ggplot point scatter
library(broom.mixed) # needed for effects plots
```

```{r setup2, include=FALSE}

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

palette5 <- c("#25CB10", "#5AB60C", "#8FA108", "#C48C04", "#FA7800")
```

# Introduction

+ What is the purpose of this project?
+ Why should we care about it?
+ What makes this a difficult exercise?
+ What is your overall modeling strategy?
+ Briefly summarize your results.

# Data

## Data Wrangling

+ Briefly describe your methods for gathering the data.

Data Sources: [Philly neighborhoods (nhoods)](https://opendataphilly.org/datasets/philadelphia-neighborhoods/), [housing sales (studentData)](https://github.com/mafichman/musa_5080_2024/blob/main/Midterm/data/2023/studentData.geojson), [crime incidents (phillyCrimes)](https://metadata.phila.gov/#home/datasetdetails/5543868920583086178c4f8e/representationdetails/570e7621c03327dc14f4b68d/)

```{r data, include = FALSE}
nhoods <- 
  st_read("data/philadelphia-neighborhoods.geojson") %>%
  st_transform('EPSG:2272')

studentData <- 
  st_read("data/studentData.geojson") %>%
  st_transform('EPSG:2272')

descriptions <- read.csv("data/data dictonary-studentdata.csv")

philly <- studentData[,c(74,75,48,3,10,11,14,15,16,17,18,19,20,21,22,23,26,34,35,38,43,44,49,50,55,58,59,60,61,65,66,69,71,72,76)]
descriptions <- descriptions %>%
  filter(Field.Name %in% colnames(philly))
```

```{r data_process}
philly <- philly %>%
  mutate(PricePerSq = philly$sale_price / philly$total_area) %>%
  mutate(Age = 2024 - year_built) %>%
  dplyr::select(- year_built)

descriptions <- rbind(descriptions,
                      data.frame(
                        Field.Name = "Age", Alias = "Age", Description = "The age of housings"
                      ))
descriptions <- descriptions %>%
  filter(Field.Name != "year_built")
```



## Feature Engineering

**Crimes: 2023-2019 (xlm)**

the ‘average nearest neighbor distance’ from each home sale to its k nearest neighbor crimes

```{r crimes_data, include = FALSE}
# phillyCrimes_2023 <- read.csv("data/Crimes/2023incidents_part1_part2.csv") %>%
#   dplyr::select(text_general_code, dispatch_date, lat, lng)
# phillyCrimes_2022 <- read.csv("data/Crimes/2022incidents_part1_part2.csv") %>%
#   dplyr::select(text_general_code, dispatch_date, lat, lng)
# phillyCrimes_2021 <- read.csv("data/Crimes/2021incidents_part1_part2.csv") %>%
#   dplyr::select(text_general_code, dispatch_date, lat, lng)
# phillyCrimes_2020 <- read.csv("data/Crimes/2020incidents_part1_part2.csv") %>%
#   dplyr::select(text_general_code, dispatch_date, lat, lng)
# phillyCrimes_2019 <- read.csv("data/Crimes/2019incidents_part1_part2.csv") %>%
#   dplyr::select(text_general_code, dispatch_date, lat, lng)
# 
# phillyCrimes <-
#   rbind(phillyCrimes_2023,phillyCrimes_2022,phillyCrimes_2021,phillyCrimes_2020,phillyCrimes_2019) %>%
#   mutate(dispatch_year = year(ymd(dispatch_date))) %>%
#   dplyr::select(lat, lng) %>%
#     na.omit() %>%
#     st_as_sf(coords = c("lng", "lat"), crs = "EPSG:4326") %>%
#     st_transform('EPSG:2272') %>%
#     distinct()
#
# st_write(phillyCrimes, dsn = 'data/phillyCrimes.geojson')

phillyCrimes <- 
  st_read("data/phillyCrimes.geojson")
```

```{r crimes_process}
philly$crimes.Buffer <- philly %>% 
    st_buffer(660) %>% 
    aggregate(mutate(phillyCrimes, counter = 1),., sum) %>%
    pull(counter)
philly <-
  philly %>% 
    mutate(
      crime_nn1 = nn_function(st_coordinates(philly), 
                              st_coordinates(phillyCrimes), k = 1),
      
      crime_nn2 = nn_function(st_coordinates(philly), 
                              st_coordinates(phillyCrimes), k = 2), 
      
      crime_nn3 = nn_function(st_coordinates(philly), 
                              st_coordinates(phillyCrimes), k = 3), 
      
      crime_nn4 = nn_function(st_coordinates(philly), 
                              st_coordinates(phillyCrimes), k = 4), 
      
      crime_nn5 = nn_function(st_coordinates(philly), 
                              st_coordinates(phillyCrimes), k = 5)) 
```

**Other data (sgz)**

## Exploratory analysis

```{r}
model_philly <- philly %>%
  filter(toPredict == "MODELLING")
```

+ Develop 1 map of your dependent variable (sale price)

```{r price_map}
ggplot() +
  geom_sf(data = nhoods, fill = "grey40", na.rm = T) +
  geom_sf(data = model_philly, aes(colour = q5(PricePerSq), na.rm = T), 
          show.legend = "point", size = .75) +
  scale_colour_manual(values = palette5,
                   labels=qBr(philly,"PricePerSq"),
                   name="Quintile\nBreaks") +
  labs(title="Price Per Square Foot, Philadelphia") +
  theme_void()
```

+ Present a table of summary statistics with variable descriptions. Sort these variables by their category (internal characteristics, amenities/public services or spatial structure).

Internal characteristics: bed, bath, area, age, garage, floor...

Amenities/public services: crime, distance to CBD/railway stations/schools/hospitals/grocery stores, green space...

The spatial process of prices: education, income, white...

(示例) references [@dubin1998predicting]

```{r}
stat_philly <- philly %>%
  dplyr::select(-musaID, -sale_price, -census_tract, -PricePerSq)

descriptions_selected <- descriptions %>%
  filter(!Field.Name %in% c("sale_price", "census_tract")) %>%
  dplyr::select(Alias, Description)
print(descriptions_selected)

stargazer(st_drop_geometry(stat_philly), 
          type = 'text', 
          title = "Summary Statistics",
          summary.stat = c("mean", "sd", "min", "max", "n"))
```

+ Include any other maps/graphs/charts you think might be of interest.

```{r}
numericVars <- 
  select_if(st_drop_geometry(model_philly), is.numeric) %>% na.omit() %>%
  dplyr::select(-musaID, -census_tract, -PricePerSq, -geographic_ward)

ggplot(gather(numericVars), aes(value)) +
  geom_histogram(bins = 50) +
  facet_wrap(~key,nrow=4, scales = 'free_x') +
  theme_minimal() +
  theme(axis.text = element_blank(), axis.ticks = element_blank())
```


```{r}
# all numeric variables
model_philly_numeric <- st_drop_geometry(model_philly) %>% 
  dplyr::select(sale_price, 
                depth, frontage, off_street_open, total_area, total_livable_area, Age) %>%
  filter(sale_price <= 1000000, Age < 500) %>%
  gather(Variable, Value, -sale_price)

ggplot(model_philly_numeric, aes(Value, sale_price)) +
  geom_point(size = .5) + geom_smooth(method = "lm", se=F, colour = "#E46726") +
  facet_wrap(~Variable, ncol = 3, scales = "free") +
  labs(title = "Price as a function of continuous variables") +
  theme_minimal()
```


```{r}
# all factor variables
model_philly_factor <- st_drop_geometry(model_philly) %>% 
  dplyr::select(sale_price, 
                basements, central_air, exterior_condition, fireplaces, fuel, garage_spaces, garage_type, general_construction,
                interior_condition, number_of_bathrooms, number_of_bedrooms, parcel_shape, quality_grade, separate_utilities, sewer,
                street_direction, topography, type_heater, view_type, building_code_description_new) %>%
  mutate(exterior_condition = as.factor(exterior_condition),
         fireplaces = as.factor(fireplaces),
         garage_spaces = as.factor(garage_spaces),
         garage_type = as.factor(garage_type),
         interior_condition = as.factor(interior_condition),
         number_of_bathrooms = as.factor(number_of_bathrooms),
         number_of_bedrooms = as.factor(number_of_bedrooms)) %>%
  filter(sale_price <= 1000000) %>%
  gather(Variable, Value, -sale_price)

ggplot(model_philly_factor, aes(Value, sale_price)) +
  geom_bar(position = "dodge", stat = "summary", fun.y = "mean") +
  facet_wrap(~Variable, ncol = 5, scales = "free") +
  labs(title = "Price as a function of categorical variables", y = "Mean Price") +
  theme_minimal() +
  theme(axis.text = element_blank(), axis.ticks = element_blank())
```

```{r}
## Crime cor
model_philly %>%
  st_drop_geometry() %>%
  dplyr::select(sale_price, starts_with("crime")) %>%
  filter(sale_price <= 1000000) %>%
  gather(Variable, Value, -sale_price) %>% 
   ggplot(aes(Value, sale_price)) +
     geom_point(size = .5) + geom_smooth(method = "lm", se=F, colour = "#E46726") +
     facet_wrap(~Variable, nrow = 1, scales = "free") +
     labs(title = "Price as a function of crimes") +
     theme_minimal()
```

+ Present a correlation matrix

this plot shows features that may be colinear, such as ...
(These two features are ‘colinear’, or correlated with one another, so if both are input into the regression, one is insignificant. In such an instance, retain the feature that leads to a more accurate and generalizable model.)

```{r}
ggcorrplot(
  round(cor(numericVars), 1), 
  p.mat = cor_pmat(numericVars),
  colors = c("#6D9EC1", "white", "#E46726"),
  type="lower",
  insig = "blank", outline.col = "grey90") +  
    labs(title = "Correlation across numeric variables") +
  theme(axis.text = element_text(size = 6))
```

(exploratory analysis goes beyond just correlation. Keep in mind that good exploratory analysis adds valuable context, particularly for non-technical audiences. )

## Feature Selection

(feature engineering like recoding Style into fewer categories, or converting NUM_FLOORS from numeric to categorical, could lead to a significant improvement in a predictive model.)

+ Present 4 home price correlation scatterplots that you think are of interest. I’m going to look for interesting open data that you’ve integrated with the home sale observations.

```{r}
st_drop_geometry(model_philly) %>% 
  dplyr::select(sale_price, total_area, total_livable_area, depth, Age) %>%
  filter(sale_price <= 1000000, Age < 500) %>%
  gather(Variable, Value, -sale_price) %>%
  ggplot(aes(Value, sale_price)) +
  geom_point(size = .5) + geom_smooth(method = "lm", se=F, colour = "#FA7800") +
  facet_wrap(~Variable, ncol = 2, scales = "free") +
  labs(title = "Price as a function of continuous variables") +
  theme_minimal()
```

+ Develop 3 maps of 3 of your most interesting independent variables.

```{r three_maps}
ggplot() +
  geom_sf(data = nhoods, fill = "transparent", color ="grey", na.rm = T) +
  geom_sf(data = philly, aes(colour = q5(Age), na.rm = T), 
          show.legend = "point", size = .75) +
  scale_colour_manual(values = palette5,
                   labels=qBr(philly,"Age"),
                   name="Quintile\nBreaks") +
  labs(title="Age of Housing, Philadelphia") +
  theme_void()

library(viridis)
ggplot() +
  geom_sf(data = nhoods, fill = "transparent", color ="grey", na.rm = T) +
  geom_sf(data = philly, aes(color = building_code_description_new, na.rm = T), 
          show.legend = "point", size = .75) +
  scale_color_viridis(discrete = TRUE, option = "D")+
  labs(title="Buidling Code Description, Philadelphia") +
  theme_void()

ggplot() +
  geom_sf(data = nhoods, fill = "transparent", color ="grey", na.rm = T) +
  geom_sf(data = philly, aes(color = factor(interior_condition), na.rm = T), 
          show.legend = "point", size = .75) +
  scale_color_brewer(palette = "RdYlBu",direction = -1)+
  labs(title="Number of Bedrooms, Philadelphia") +
  theme_void()

```

**?? pearson correlation**


```{r}
cor_philly <- 
  st_drop_geometry(model_philly) %>%
  filter(sale_price <= 1000000,
         total_livable_area < 10000) %>%
  dplyr::select(sale_price, total_livable_area)

cor.test(cor_philly$total_livable_area,
         cor_philly$sale_price, 
         method = "pearson")


# Calculate predicted prices
cor_philly$predicted_price <- predict(lm(sale_price ~ total_livable_area, data = cor_philly), newdata = cor_philly)
ggscatter(cor_philly,
          x = "total_livable_area",
          y = "sale_price",
          color = "#6D9EC1", size = 1, alpha = 0.6) +
  geom_point(aes(y = predicted_price), color = "#E46726", size = 1, alpha = 0.6) +
  stat_cor(label.x = 4000, label.y = 250000, hjust = 0) +
  labs(title = "Price as a function of living area", 
       subtitle = "With predicted prices; Sale prices <= $1 mil.",
       x = "Total Livable Area",
       y = "Sale Price") +
  theme_minimal() +
  theme(plot.title = element_text(size = 18, face = "bold"),
        plot.subtitle = element_text(size = 12)) 
```


# Methods

Briefly describe your method (remember who your audience is).

a)  data cleaning
b)	Exploratory Data Analysis
c)  OLS Regression
d)  additional analyses (k-fold cross-validation, Moran's I...)
e)  software

# Results: Briefly interpret each in the context of the Zillow use case

## Ordinary Least Squares regression

+ Split the ‘toPredict’ == “MODELLING” into a training and test set. Provide a polished table of your training set lm summary results (coefficients, R2 etc).

**paste种类和数量少的variable**

```{r}
model_philly <- model_philly %>%
  mutate(exterior_condition = as.factor(exterior_condition),
         fireplaces = as.factor(fireplaces),
         garage_spaces = as.factor(garage_spaces),
         garage_type = as.factor(garage_type),
         interior_condition = as.factor(interior_condition),
         number_of_bathrooms = as.factor(number_of_bathrooms),
         number_of_bedrooms = as.factor(number_of_bedrooms))

inTrain <- createDataPartition(
              y = paste(model_philly$total_livable_area, model_philly$interior_condition,
                        model_philly$building_code_description_new, model_philly$census_tract), 
              p = .60, list = FALSE)
# [row,column] select all columns
philly.training <- model_philly[inTrain,] 
philly.test <- model_philly[-inTrain,]
```

```{r}
reg.training <- lm(sale_price ~.,data = st_drop_geometry(philly.training) %>%
                  dplyr::select(sale_price, depth, frontage, off_street_open, total_area, total_livable_area, Age, crime_nn3,
                                basements, central_air, exterior_condition, fireplaces, fuel, garage_spaces, garage_type, 
                                general_construction,interior_condition, number_of_bathrooms, number_of_bedrooms, parcel_shape, 
                                quality_grade, separate_utilities, sewer, street_direction, topography, type_heater, view_type, 
                                building_code_description_new))

stargazer(reg.training, type = "text",title="Regression Results",align=TRUE,no.space=TRUE)
```

## Accuracy - Mean Absolute Error

+ Provide a polished table of mean absolute error (MAE) and MAPE for a single test set. 

```{r}
philly.test <-
  philly.test %>%
  mutate(Regression = "Baseline Regression",
         sale_price.Predict = predict(reg.training, philly.test),
         sale_price.Error = sale_price.Predict - sale_price,
         sale_price.AbsError = abs(sale_price.Predict - sale_price),
         sale_price.APE = (abs(sale_price.Predict - sale_price)) / sale_price.Predict) %>%
  filter(sale_price < 5000000)

MAE <- mean(philly.test$sale_price.AbsError, na.rm = T)
MAPE <- mean(philly.test$sale_price.APE, na.rm = T)
acc <- data.frame(MAE, MAPE)
kable(acc) %>% kable_styling(full_width = F)
```

+ Plot predicted prices as a function of observed prices

```{r}
ggplot(
  philly.test, aes(sale_price.Predict, sale_price)) +
  geom_point(size = .5) + 
  geom_smooth(method = "lm", se=F, colour = "#6D9EC1") +
  geom_abline(intercept = 0, slope = 1, color="#E46726",size=1) +
  labs(title = 'Predicted sale price as a function of observed price', subtitle = 'Red line represents a perfect prediction;\nBlue line represents prediction') +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0, size = 16),
        plot.subtitle = element_text(size = 10, face = "italic"))
```

## Generalizability - Cross-validation

+ Provide the results of your cross-validation tests. This includes mean and standard deviation MAE. Do 100 folds and plot your cross-validation MAE as a histogram. Is your model generalizable to new data?

```{r}
fitControl <- trainControl(method = "cv", number = 100)
set.seed(825)

reg.cv <- 
  train(sale_price ~ ., data = st_drop_geometry(model_philly) %>% 
          dplyr::select(sale_price, depth, frontage, off_street_open, total_area, total_livable_area, Age, crime_nn3,
                                basements, central_air, exterior_condition, fireplaces, fuel, garage_spaces, garage_type, 
                                general_construction,interior_condition, number_of_bathrooms, number_of_bedrooms, parcel_shape, 
                                quality_grade, separate_utilities, sewer, street_direction, topography, type_heater, view_type, 
                                building_code_description_new),
        method = "lm", trControl = fitControl, na.action = na.pass)

reg.cv
```

```{r}
Mean <- mean(reg.cv$resample[,3])
SD <- sd(reg.cv$resample[,3])
stat_MAE <- data.frame(Mean, SD)
kable(stat_MAE) %>% kable_styling(full_width = F)
```

**改用ggplot**

(If the model generalized well, the distribution of errors would cluster tightly together.)

```{r}
hist(reg.cv$resample[,3], 
     breaks = 50, 
     main = 'Distribution of MAE \nK fold cross validation (k=100)', 
     xlab = 'Mean Absolute Error', 
     ylab = 'count')
```

+ Provide a map of your residuals for your test set. Include a Moran’s I test and a plot of the spatial lag in errors. (卡住了)

```{r}
coords <- st_coordinates(model_philly) 
# create a list of 5 nearest neighbors
neighborList <- knn2nb(knearneigh(coords, 5))

spatialWeights <- nb2listw(neighborList, style="W")
model_philly$lagPrice <- lag.listw(spatialWeights, model_philly$sale_price)

coords.test <-  st_coordinates(philly.test) 
neighborList.test <- knn2nb(knearneigh(coords.test, 5))
spatialWeights.test <- nb2listw(neighborList.test, style="W")
# compare 
philly.test %>% 
  mutate(lagPriceError = lag.listw(spatialWeights.test, sale_price.Error)) %>%
  ggplot(aes(x =lagPriceError, y =sale_price.Error))+
  geom_point(aes(x =lagPriceError, y =sale_price.Error), color = "orange") +
  labs(title = "Spatial Lag of Sale Price Error",
       x = "Lag of Sale Price Error",
       y = "Sale Price Error") +
  geom_smooth(method="lm")
```


```{r}
moranTest <- moran.mc(philly.test_predict$price.Error, 
                      spatialWeights.test, nsim = 999)

ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = moranTest$statistic), colour = "#FA7800",size=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  labs(title="Observed and permuted Moran's I",
       subtitle= "Observed Moran's I in orange",
       x="Moran's I",
       y="Count") +
  plotTheme()
```


+ Provide a map of your predicted values for where ‘toPredict’ is both “MODELLING” and “CHALLENGE”.

```{r}

```


+ Using the test set predictions, provide a map of mean absolute percentage error (MAPE) by neighborhood.
+ Provide a scatterplot plot of MAPE by neighborhood as a function of mean price by neighborhood.
+ Using tidycensus, split your study area into two groups (perhaps by race or income) and test your model’s generalizability. Is your model generalizable?

# Discussion

Is this an effective model? What were some of the more interesting variables?  How much of the variation in prices could you predict? Describe the more important features? Describe the error in your predictions?  According to your maps, could you account the spatial variation in prices?  Where did the model predict particularly well? Poorly? Why do you think this might be?

# Conclusion

Would you recommend your model to Zillow? Why or why not? How might you improve this model?

# References





