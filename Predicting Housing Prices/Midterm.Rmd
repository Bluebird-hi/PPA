---
title: "Predicting housing prices"
author: "Guangze Sun & Luming Xu"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    theme: spacelab
    toc_float: true
    toc_depth: 2
    code_folding: hide
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE) 
options(scipen=999)

# Load some libraries
library(tidyverse)
library(dplyr)
library(lubridate)
library(sf)
library(stargazer)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(ggcorrplot) # plot correlation plot
library(corrr)      # another way to plot correlation plot
library(kableExtra)
library(jtools)     # for regression model plots
library(ggstance) # to support jtools plots
library(ggpubr)    # plotting R^2 value on ggplot point scatter
library(broom.mixed) # needed for effects plots
```

```{r setup2, include=FALSE}

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

palette5 <- c("#25CB10", "#5AB60C", "#8FA108", "#C48C04", "#FA7800")
```

# Introduction

+ What is the purpose of this project?
+ Why should we care about it?
+ What makes this a difficult exercise?
+ What is your overall modeling strategy?
+ Briefly summarize your results.

# Data

## Data Wrangling

+ Briefly describe your methods for gathering the data.

Data Sources: [Philly neighborhoods (nhoods)](https://opendataphilly.org/datasets/philadelphia-neighborhoods/), [housing sales (studentData)](https://github.com/mafichman/musa_5080_2024/blob/main/Midterm/data/2023/studentData.geojson), [crime incidents (phillyCrimes)](https://metadata.phila.gov/#home/datasetdetails/5543868920583086178c4f8e/representationdetails/570e7621c03327dc14f4b68d/)

```{r data, include = FALSE}
nhoods <- 
  st_read("data/philadelphia-neighborhoods.geojson") %>%
  st_transform('EPSG:2272')

studentData <- 
  st_read("data/studentData.geojson") %>%
  st_transform('EPSG:2272')

philly <- studentData[,c(74,75,48,3,10,11,14,15,16,17,18,19,20,21,22,23,26,34,35,38,43,44,49,50,55,58,59,60,61,65,66,69,71,72,76)]
```

```{r data_process}
philly <- philly %>%
  mutate(PricePerSq = philly$sale_price / philly$total_area) %>%
  mutate(Age = 2024 - year_built) %>%
  dplyr::select(- year_built)
```

**Crimes: 2023-2019 (xlm)**

```{r crimes_data, include = FALSE}
# phillyCrimes_2023 <- read.csv("data/Crimes/2023incidents_part1_part2.csv") %>%
#   dplyr::select(text_general_code, dispatch_date, lat, lng)
# phillyCrimes_2022 <- read.csv("data/Crimes/2022incidents_part1_part2.csv") %>%
#   dplyr::select(text_general_code, dispatch_date, lat, lng)
# phillyCrimes_2021 <- read.csv("data/Crimes/2021incidents_part1_part2.csv") %>%
#   dplyr::select(text_general_code, dispatch_date, lat, lng)
# phillyCrimes_2020 <- read.csv("data/Crimes/2020incidents_part1_part2.csv") %>%
#   dplyr::select(text_general_code, dispatch_date, lat, lng)
# phillyCrimes_2019 <- read.csv("data/Crimes/2019incidents_part1_part2.csv") %>%
#   dplyr::select(text_general_code, dispatch_date, lat, lng)
# 
# phillyCrimes <-
#   rbind(phillyCrimes_2023,phillyCrimes_2022,phillyCrimes_2021,phillyCrimes_2020,phillyCrimes_2019) %>%
#   mutate(dispatch_year = year(ymd(dispatch_date))) %>%
#   dplyr::select(lat, lng) %>%
#     na.omit() %>%
#     st_as_sf(coords = c("lng", "lat"), crs = "EPSG:4326") %>%
#     st_transform('EPSG:2272') %>%
#     distinct()
#
# st_write(phillyCrimes, dsn = 'data/phillyCrimes.geojson')

phillyCrimes <- 
  st_read("data/phillyCrimes.geojson")
```

```{r crimes_process}
philly$crimes.Buffer <- philly %>% 
    st_buffer(660) %>% 
    aggregate(mutate(phillyCrimes, counter = 1),., sum) %>%
    pull(counter)
philly <-
  philly %>% 
    mutate(
      crime_nn1 = nn_function(st_coordinates(philly), 
                              st_coordinates(phillyCrimes), k = 1),
      
      crime_nn2 = nn_function(st_coordinates(philly), 
                              st_coordinates(phillyCrimes), k = 2), 
      
      crime_nn3 = nn_function(st_coordinates(philly), 
                              st_coordinates(phillyCrimes), k = 3), 
      
      crime_nn4 = nn_function(st_coordinates(philly), 
                              st_coordinates(phillyCrimes), k = 4), 
      
      crime_nn5 = nn_function(st_coordinates(philly), 
                              st_coordinates(phillyCrimes), k = 5)) 
```

**Other data (sgz)**

## Exploratory analysis

```{r}
model_philly <- philly %>%
  filter(toPredict == "MODELLING")
```


+ Present a table of summary statistics with variable descriptions. Sort these variables by their category (internal characteristics, amenities/public services or spatial structure).

Internal characteristics: bed, bath, area, age, garage, floor...

Amenities/public services: distance to CBD/railway stations/schools/hospitals/grocery stores, green space...

Tract characteristics: education, income, white...

(示例) references [@dubin1998predicting]

```{r}
stat_philly <- philly %>%
  dplyr::select(-musaID, -sale_price, -census_tract)

stargazer(st_drop_geometry(stat_philly), 
          type = 'text', 
          title = "Summary Statistics")
```

+ Present a correlation matrix

```{r}
numericVars <- 
  select_if(st_drop_geometry(model_philly), is.numeric) %>% na.omit() %>%
  dplyr::select(-musaID, -census_tract)

ggcorrplot(
  round(cor(numericVars), 1), 
  p.mat = cor_pmat(numericVars),
  colors = c("#25CB10", "white", "#FA7800"),
  type="lower",
  insig = "blank") +  
    labs(title = "Correlation across numeric variables") 
```


+ Present 4 home price correlation scatterplots that you think are of interest. I’m going to look for interesting open data that you’ve integrated with the home sale observations.

```{r}
st_drop_geometry(model_philly) %>% 
  dplyr::select(sale_price, total_area, total_livable_area, number_of_bedrooms, Age) %>%
  filter(sale_price <= 1000000, Age < 500) %>%
  gather(Variable, Value, -sale_price) %>% 
   ggplot(aes(Value, sale_price)) +
     geom_point(size = .5) + geom_smooth(method = "lm", se=F, colour = "#FA7800") +
     facet_wrap(~Variable, ncol = 2, scales = "free") +
     labs(title = "Price as a function of continuous variables") +
     plotTheme()
```

+ Develop 1 map of your dependent variable (sale price)

```{r price_map}
ggplot() +
  geom_sf(data = nhoods, fill = "grey40", na.rm = T) +
  geom_sf(data = model_philly, aes(colour = q5(PricePerSq), na.rm = T), 
          show.legend = "point", size = .75) +
  scale_colour_manual(values = palette5,
                   labels=qBr(philly,"PricePerSq"),
                   name="Quintile\nBreaks") +
  labs(title="Price Per Square Foot, Philadelphia") +
  theme_void()
```

+ Develop 3 maps of 3 of your most interesting independent variables.

```{r three_maps}
ggplot() +
  geom_sf(data = nhoods, fill = "transparent", color ="grey", na.rm = T) +
  geom_sf(data = philly, aes(colour = q5(Age), na.rm = T), 
          show.legend = "point", size = .75) +
  scale_colour_manual(values = palette5,
                   labels=qBr(philly,"Age"),
                   name="Quintile\nBreaks") +
  labs(title="Age of Housing, Philadelphia") +
  theme_void()

library(viridis)
ggplot() +
  geom_sf(data = nhoods, fill = "transparent", color ="grey", na.rm = T) +
  geom_sf(data = philly, aes(color = building_code_description_new, na.rm = T), 
          show.legend = "point", size = .75) +
  scale_color_viridis(discrete = TRUE, option = "D")+
  labs(title="Buidling Code Description, Philadelphia") +
  theme_void()

ggplot() +
  geom_sf(data = nhoods, fill = "transparent", color ="grey", na.rm = T) +
  geom_sf(data = philly, aes(color = factor(interior_condition), na.rm = T), 
          show.legend = "point", size = .75) +
  scale_color_brewer(palette = "RdYlBu")+
  labs(title="Number of Bedrooms, Philadelphia") +
  theme_void()

```


+ Include any other maps/graphs/charts you think might be of interest.

crimes,...

```{r}
## Crime cor
model_philly %>%
  st_drop_geometry() %>%
  dplyr::select(sale_price, starts_with("crime")) %>%
  filter(sale_price <= 1000000) %>%
  gather(Variable, Value, -sale_price) %>% 
   ggplot(aes(Value, sale_price)) +
     geom_point(size = .5) + geom_smooth(method = "lm", se=F, colour = "#FA7800") +
     facet_wrap(~Variable, nrow = 2, scales = "free") +
     labs(title = "Price as a function of crimes") +
     theme_minimal()
```

```{r}
ggplot(gather(numericVars), aes(value)) +
  geom_histogram(bins = 50) +
  facet_wrap(~key,nrow=6, scales = 'free_x')
```

```{r}
cor_philly <- 
  st_drop_geometry(model_philly) %>%
  filter(sale_price <= 1000000,
         total_livable_area < 10000)

cor.test(cor_philly$total_livable_area,
         cor_philly$sale_price, 
         method = "pearson")

ggscatter(cor_philly,
          x = "total_livable_area",
          y = "sale_price",
          add = "reg.line") +
  stat_cor(label.y = 1200000) 
```


# Methods

Briefly describe your method (remember who your audience is).

a)  data cleaning
b)	Exploratory Data Analysis
c)  OLS Regression
d)  additional analyses (k-fold cross-validation, Moran's I...)
e)  software

# Results: Briefly interpret each in the context of the Zillow use case

+ Split the ‘toPredict’ == “MODELLING” into a training and test set. Provide a polished table of your training set lm summary results (coefficients, R2 etc).

```{r}
inTrain <- createDataPartition(
              y = paste(model_philly$total_livable_area, model_philly$interior_condition,
                        model_philly$building_code_description_new, model_philly$census_tract), 
              p = .60, list = FALSE)
# [row,column] select all columns
philly.training <- model_philly[inTrain,] 
philly.test <- model_philly[-inTrain,]
```

```{r}
all_reg <- lm(sale_price ~.,data = st_drop_geometry(philly.training) %>%
                  dplyr::select(sale_price, total_livable_area, interior_condition))

stargazer(all_reg, type = "text",title="Regression Results",align=TRUE,no.space=TRUE)
```

+ Provide a polished table of mean absolute error and MAPE for a single test set. 

```{r}
philly.test <-
  philly.test %>%
  mutate(Regression = "Baseline Regression",
         sale_price.Predict = predict(all_reg, philly.test),
         sale_price.Error = sale_price.Predict - sale_price,
         sale_price.AbsError = abs(sale_price.Predict - sale_price),
         sale_price.APE = (abs(sale_price.Predict - sale_price)) / sale_price) %>%
  filter(sale_price < 5000000)

MAE <- mean(philly.test$sale_price.AbsError, na.rm = T)
MAPE <- mean(philly.test$sale_price.APE, na.rm = T)
acc <- data.frame(MAE, MAPE)
kable(acc) %>% kable_styling(full_width = F)
```


+ Provide the results of your cross-validation tests. This includes mean and standard deviation MAE. Do 100 folds and plot your cross-validation MAE as a histogram. Is your model generalizable to new data?

```{r}
fitControl <- trainControl(method = "cv", number = 100)
set.seed(825)

all_reg.cv <- 
  train(sale_price ~ ., data = st_drop_geometry(model_philly) %>% 
          dplyr::select(sale_price, total_livable_area, interior_condition),
        method = "lm", trControl = fitControl, na.action = na.pass)

all_reg.cv
```

```{r}
Mean <- mean(all_reg.cv$resample[,3])
SD <- sd(all_reg.cv$resample[,3])
stat_MAE <- data.frame(Mean, SD)
kable(stat_MAE) %>% kable_styling(full_width = F)
```

```{r}
hist(all_reg.cv$resample[,3], 
     breaks = 50, 
     main = 'Distribution of MAE \nK fold cross validation (k=100)', 
     xlab = 'Mean Absolute Error', 
     ylab = 'count')
```

+ Plot predicted prices as a function of observed prices

```{r}
ggplot(
  philly.test, aes(sale_price.Predict, sale_price)) +
  geom_point(size = .5) + 
  geom_smooth(method = "lm", se=F, colour = "#018abe") +
  geom_abline(intercept = 0, slope = 1, color='red',size=1) +
  labs(title = 'Predicted sale price as a function of observed price', subtitle = 'Red line represents a perfect prediction \nBlue line represents prediction') +
  theme(plot.title = element_text(hjust = 0.5))
```

+ Provide a map of your residuals for your test set. Include a Moran’s I test and a plot of the spatial lag in errors. (卡住了)

```{r}
coords <- st_coordinates(model_philly) 
# create a list of 5 nearest neighbors
neighborList <- knn2nb(knearneigh(coords, 5))

spatialWeights <- nb2listw(neighborList, style="W")
model_philly$lagPrice <- lag.listw(spatialWeights, model_philly$sale_price)

coords.test <-  st_coordinates(philly.test) 
neighborList.test <- knn2nb(knearneigh(coords.test, 5))
spatialWeights.test <- nb2listw(neighborList.test, style="W")
# compare 
philly.test %>% 
  mutate(lagPriceError = lag.listw(spatialWeights.test, sale_price.Error)) %>%
  ggplot(aes(x =lagPriceError, y =sale_price.Error))+
  geom_point(aes(x =lagPriceError, y =sale_price.Error), color = "orange") +
  labs(title = "Spatial Lag of Sale Price Error",
       x = "Lag of Sale Price Error",
       y = "Sale Price Error") +
  geom_smooth(method="lm")
```


+ Provide a map of your predicted values for where ‘toPredict’ is both “MODELLING” and “CHALLENGE”.
+ Using the test set predictions, provide a map of mean absolute percentage error (MAPE) by neighborhood.
+ Provide a scatterplot plot of MAPE by neighborhood as a function of mean price by neighborhood.
+ Using tidycensus, split your study area into two groups (perhaps by race or income) and test your model’s generalizability. Is your model generalizable?

# Discussion

Is this an effective model? What were some of the more interesting variables?  How much of the variation in prices could you predict? Describe the more important features? Describe the error in your predictions?  According to your maps, could you account the spatial variation in prices?  Where did the model predict particularly well? Poorly? Why do you think this might be?

# Conclusion

Would you recommend your model to Zillow? Why or why not? How might you improve this model?

# References





